import os
from openai import OpenAI
import numpy as np
import json
from typing import Dict, List, Tuple, Optional, Union
from config import *
from dataUtils import *
from embeddingUtils import *
from storeUtils import *
from pipelines import *
from basicRl import *

# Âä†ËΩΩÊï∞ÊçÆÊñá‰ª∂
# Load all text documents from the specified directory
documents = load_documents(directory_path)
# Split the loaded documents into smaller chunks of text
chunks = split_into_chunks(documents)
# Preprocess the chunks (e.g., lowercasing, removing special characters)
preprocessed_chunks = preprocess_chunks(chunks)
print(f"Total number of chunks: {len(preprocessed_chunks)}")
for i in range(2):
    # Use slicing to limit the output to the first 200
    print(f"Chunk {i+1}: {preprocessed_chunks[i][:50]} ... ")
    print("-" * 50)  # Print a separator line


# Â§ÑÁêÜembeddingÊñá‰ª∂
# Ensure the chunks are preprocessed before generating embeddings
preprocessed_chunks = preprocess_chunks(chunks)
# Generate embeddings for the preprocessed chunks
embeddings = generate_embeddings(preprocessed_chunks)
# Save the generated embeddings to a JSON file named "embeddings.json"
save_embeddings(embeddings, "embeddings.json")
print(preprocessed_chunks[0:2])
print(embeddings[0:2])


# Â≠òÂÇ®embeddingÊñá‰ª∂
# Add the generated embeddings and their corresponding preprocessed chunks to the vector store
add_to_vector_store(embeddings, preprocessed_chunks)
# Define a query text for which we want to retrieve relevant document chunks
query_text = "What is Quantum Computing?"
# Retrieve the most relevant chunks from the vector store based on the query text
relevant_chunks = retrieve_relevant_chunks(query_text)
# Print the first 50 characters of each retrieved relevant chunk
for idx, chunk in enumerate(relevant_chunks):
    print(f"Chunk {idx + 1}: {chunk[:50]} ... ")
    print("-" * 50)  # Print a separator line


# ÊµãËØïbasic pipeline
# Open the validation data file in read mode and load its content as a dictionary
with open('data/val.json', 'r') as file:
    validation_data = json.load(file)
# Test the basic RAG pipeline with a sample query
sample_query = validation_data['basic_factual_questions'][0]['question']  # Extract the query text
expected_answer = validation_data['basic_factual_questions'][0]['answer']  # Extract the ground truth answer
# print the sample query and expected answer
print(f"Sample Query: {sample_query}\n")
print(f"Expected Answer: {expected_answer}\n")

# Print a message to indicate the start of the RAG pipeline
print("üîç Running the Retrieval-Augmented Generation (RAG) pipeline...")
print(f"üì• Query: {sample_query}\n")

# Run the RAG pipeline and get the response
response = basic_rag_pipeline(sample_query)

# Print the response with better formatting
print("ü§ñ AI Response:")
print("-" * 50)
print(response.strip())
print("-" * 50)

# Print the ground truth answer for comparison
print("‚úÖ Ground Truth Answer:")
print("-" * 50)
print(expected_answer)
print("-" * 50)




# ÊµãËØï rl rag
# Compare the performance of the simple RAG pipeline and the RL-enhanced RAG pipeline
# using the sample query and its expected answer.
# The function returns:
# - simple_response: The response generated by the simple RAG pipeline.
# - rl_response: The best response generated by the RL-enhanced RAG pipeline.
# - simple_sim: The similarity score of the simple RAG response to the ground truth.
# - rl_sim: The similarity score of the RL-enhanced RAG response to the ground truth.
simple_response, rl_response, simple_sim, rl_sim = compare_rag_approaches(sample_query, expected_answer)